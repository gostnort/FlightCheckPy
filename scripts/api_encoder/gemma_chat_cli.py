#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gemma3èŠå¤©å‘½ä»¤è¡Œç•Œé¢
æä¾›å®Œæ•´çš„å¯¹è¯ä½“éªŒï¼Œæ”¯æŒå¤šç§Gemmaæ¨¡å‹ï¼Œæ˜¾ç¤ºtokenä½¿ç”¨æƒ…å†µ
"""

import google.generativeai as genai
import threading
import time
import sys
from typing import Optional
from api_encoder import APIEncoder


class ChatInterface:
    """Gemma3èŠå¤©å‘½ä»¤è¡Œç•Œé¢ç±»"""
    
    
    def __init__(self):
        """åˆå§‹åŒ–èŠå¤©ç•Œé¢"""
        self.api_key = None
        self.username = None
        self.model = None
        self.model_name = None
        self.conversation_history = []
        self.max_input_tokens = 128000  # é»˜è®¤è¾“å…¥tokené™åˆ¶
        self.max_output_tokens = 8000   # é»˜è®¤è¾“å‡ºtokené™åˆ¶
        self.current_tokens = 0
        
        # å¯ç”¨çš„Gemmaæ¨¡å‹é…ç½® (ä»…Googleå®˜æ–¹æ¨¡å‹)
        self.available_models = {
            "1": {
                "name": "gemma-3-1b-it", 
                "display": "Gemma 3 1B IT (æ–‡æœ¬ç”Ÿæˆï¼Œè½»é‡å¿«é€Ÿ)",
                "max_input_tokens": 128000,
                "max_output_tokens": 8000,
                "optimization": "æ–‡æœ¬ç”Ÿæˆä¼˜åŒ–"
            },
            "2": {
                "name": "gemma-3-270m-it",
                "display": "Gemma 3 270M IT (è¶…è½»é‡ï¼Œç§»åŠ¨è®¾å¤‡ä¼˜åŒ–)",
                "max_input_tokens": 128000,
                "max_output_tokens": 8000,
                "optimization": "ç§»åŠ¨è®¾å¤‡ä¼˜åŒ–"
            },
            "3": {
                "name": "gemma-3-4b-it",
                "display": "Gemma 3 4B IT (å›¾åƒæ–‡æœ¬è½¬æ¢ï¼Œä¸­ç­‰æ€§èƒ½)", 
                "max_input_tokens": 128000,
                "max_output_tokens": 8000,
                "optimization": "å›¾åƒæ–‡æœ¬å¤„ç†ä¼˜åŒ–"
            },
            "4": {
                "name": "gemma-3-12b-it",
                "display": "Gemma 3 12B IT (å›¾åƒæ–‡æœ¬è½¬æ¢ï¼Œé«˜æ€§èƒ½)",
                "max_input_tokens": 128000,
                "max_output_tokens": 8000,
                "optimization": "å›¾åƒæ–‡æœ¬å¤„ç†ä¼˜åŒ–"
            },
            "5": {
                "name": "gemma-3-27b-it",
                "display": "Gemma 3 27B IT (å›¾åƒæ–‡æœ¬è½¬æ¢ï¼Œæœ€é«˜æ€§èƒ½)",
                "max_input_tokens": 128000,
                "max_output_tokens": 8000,
                "optimization": "å›¾åƒæ–‡æœ¬å¤„ç†ä¼˜åŒ–"
            },
            "6": {
                "name": "gemma-3n-E4B-it",
                "display": "Gemma 3N E4B IT (å›¾åƒæ–‡æœ¬è½¬æ¢ï¼Œå®éªŒç‰ˆ)",
                "max_input_tokens": 32000,
                "max_output_tokens": 32000,
                "optimization": "å›¾åƒæ–‡æœ¬å¤„ç†å®éªŒ"
            },
            "7": {
                "name": "gemma-3n-E4B-it-litert-lm",
                "display": "Gemma 3N E4B LiteRT LM (æ–‡æœ¬ç”Ÿæˆï¼Œè½»é‡è¿è¡Œæ—¶)",
                "max_input_tokens": 32000,
                "max_output_tokens": 32000,
                "optimization": "è½»é‡è¿è¡Œæ—¶ä¼˜åŒ–"
            },
            "8": {
                "name": "gemma-3n-E4B-it-litert-preview",
                "display": "Gemma 3N E4B LiteRT Preview (å›¾åƒæ–‡æœ¬è½¬æ¢ï¼Œé¢„è§ˆç‰ˆ)",
                "max_input_tokens": 32000,
                "max_output_tokens": 32000,
                "optimization": "å›¾åƒæ–‡æœ¬å¤„ç†é¢„è§ˆ"
            }
        }


    def setup_authentication(self) -> bool:
        """
        è®¾ç½®ç”¨æˆ·è®¤è¯
        Returns:
            è®¤è¯æ˜¯å¦æˆåŠŸ
        """
        print("=== Gemma3 èŠå¤©ç•Œé¢ ===")
        print("è¯·è¾“å…¥æ‚¨çš„ç”¨æˆ·åè¿›è¡Œèº«ä»½éªŒè¯")
        print()
        
        username = input("ç”¨æˆ·å: ").strip()
        if not username:
            print("âŒ ç”¨æˆ·åä¸èƒ½ä¸ºç©º")
            return False
            
        # éªŒè¯APIå¯†é’¥
        encoder = APIEncoder()
        api_key = encoder.decode_api_key(username)
        if not api_key:
            print("âŒ ç”¨æˆ·åéªŒè¯å¤±è´¥ï¼Œæ— æ³•è·å–APIå¯†é’¥")
            return False
            
        self.username = username
        self.api_key = api_key
        print(f"âœ… ç”¨æˆ·éªŒè¯æˆåŠŸï¼æ¬¢è¿, {username}")
        print()
        return True


    def setup_model_selection(self) -> bool:
        """
        è®¾ç½®æ¨¡å‹é€‰æ‹©
        Returns:
            æ¨¡å‹é€‰æ‹©æ˜¯å¦æˆåŠŸ
        """
        print("è¯·é€‰æ‹©è¦ä½¿ç”¨çš„Gemmaæ¨¡å‹:")
        print()
        
        for key, model_info in self.available_models.items():
            print(f"{key}. {model_info['display']}")
        print()
        
        while True:
            choice = input("è¯·é€‰æ‹©æ¨¡å‹ (1-8): ").strip()
            if choice in self.available_models:
                model_info = self.available_models[choice]
                self.model_name = model_info["name"]
                self.max_input_tokens = model_info["max_input_tokens"]
                self.max_output_tokens = model_info["max_output_tokens"]
                
                try:
                    # é…ç½®APIå¹¶åˆ›å»ºæ¨¡å‹
                    genai.configure(api_key=self.api_key)
                    self.model = genai.GenerativeModel(self.model_name)
                    print(f"âœ… å·²é€‰æ‹©æ¨¡å‹: {model_info['display']}")
                    print(f"ğŸ“Š è¾“å…¥Tokené™åˆ¶: {self.max_input_tokens:,}")
                    print(f"ğŸ“Š è¾“å‡ºTokené™åˆ¶: {self.max_output_tokens:,}")
                    print()
                    return True
                except Exception as e:
                    print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
                    return False
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-8ä¹‹é—´çš„æ•°å­—")


    def estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
        Args:
            text: è¾“å…¥æ–‡æœ¬
        Returns:
            ä¼°ç®—çš„tokenæ•°é‡
        """
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦*1.5 + è‹±æ–‡å•è¯*1.3
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        english_words = len([w for w in text.split() if any(c.isalpha() for c in w)])
        other_chars = len(text) - chinese_chars - sum(len(w) for w in text.split() if any(c.isalpha() for c in w))
        
        estimated = int(chinese_chars * 1.5 + english_words * 1.3 + other_chars * 0.5)
        return max(estimated, len(text) // 4)  # æœ€ä½ä¼°ç®—ä¸ºå­—ç¬¦æ•°çš„1/4


    def update_token_count(self):
        """æ›´æ–°å½“å‰å¯¹è¯çš„tokenæ•°é‡"""
        total_text = ""
        for msg in self.conversation_history:
            total_text += msg["content"] + " "
        self.current_tokens = self.estimate_tokens(total_text)


    def display_token_info(self):
        """æ˜¾ç¤ºtokenä½¿ç”¨ä¿¡æ¯"""
        input_percentage = (self.current_tokens / self.max_input_tokens) * 100
        bar_length = 30
        filled_length = int(bar_length * input_percentage / 100)
        bar = "â–ˆ" * filled_length + "â–‘" * (bar_length - filled_length)
        
        print(f"\nğŸ“Š è¾“å…¥Tokenä½¿ç”¨æƒ…å†µ: {self.current_tokens:,}/{self.max_input_tokens:,} ({input_percentage:.1f}%)")
        print(f"[{bar}]")
        print(f"ğŸ“¤ è¾“å‡ºTokené™åˆ¶: {self.max_output_tokens:,}")
        
        if input_percentage > 90:
            print("âš ï¸  è­¦å‘Š: è¾“å…¥Tokenä½¿ç”¨æ¥è¿‘ä¸Šé™ï¼Œè€ƒè™‘æ¸…ç†å¯¹è¯å†å²")
        elif input_percentage > 70:
            print("ğŸ’¡ æç¤º: è¾“å…¥Tokenä½¿ç”¨è¾ƒå¤šï¼Œå»ºè®®é€‚åº¦æ§åˆ¶å¯¹è¯é•¿åº¦")


    def build_conversation_context(self) -> str:
        """
        æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        Returns:
            å®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not self.conversation_history:
            return ""
            
        context_parts = ["ä»¥ä¸‹æ˜¯æˆ‘ä»¬çš„å¯¹è¯å†å²ï¼š\n"]
        for msg in self.conversation_history[-10:]:  # åªä¿ç•™æœ€è¿‘10è½®å¯¹è¯
            role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
            context_parts.append(f"{role}: {msg['content']}\n")
        
        context_parts.append("\nè¯·æ ¹æ®ä»¥ä¸Šå¯¹è¯å†å²å›ç­”æ–°çš„é—®é¢˜ã€‚")
        return "".join(context_parts)


    def send_message_to_gemma(self, user_input: str) -> Optional[str]:
        """
        å‘é€æ¶ˆæ¯åˆ°Gemmaå¹¶è·å–å›å¤
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
        Returns:
            Gemmaçš„å›å¤æˆ–None
        """
        try:
            # æ„å»ºå®Œæ•´çš„prompt
            context = self.build_conversation_context()
            full_prompt = f"{context}\n\nç”¨æˆ·æ–°é—®é¢˜: {user_input}"
            
            # é…ç½®ç”Ÿæˆå‚æ•°ï¼Œä½¿ç”¨æ¨¡å‹çš„å®é™…è¾“å‡ºé™åˆ¶
            generation_config = {
                'temperature': 0.7,
                'max_output_tokens': min(2048, self.max_output_tokens),
                'top_p': 0.9,
                'top_k': 40
            }
            
            # ä½¿ç”¨å¤šçº¿ç¨‹å®ç°è¶…æ—¶
            result = [None]
            exception = [None]
            
            def api_call():
                try:
                    response = self.model.generate_content(
                        full_prompt, 
                        generation_config=generation_config
                    )
                    result[0] = response.text.strip()
                except Exception as e:
                    exception[0] = e
            
            # æ˜¾ç¤ºæ­£åœ¨æ€è€ƒçš„åŠ¨ç”»
            thread = threading.Thread(target=api_call)
            thread.start()
            
            # ç­‰å¾…å“åº”ï¼Œæ˜¾ç¤ºåŠ è½½åŠ¨ç”»
            loading_chars = "â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â "
            char_index = 0
            
            while thread.is_alive():
                print(f"\rğŸ¤– Gemmaæ­£åœ¨æ€è€ƒ {loading_chars[char_index]}", end="", flush=True)
                char_index = (char_index + 1) % len(loading_chars)
                time.sleep(0.1)
                
            thread.join(timeout=15.0)  # 15ç§’è¶…æ—¶
            print("\r" + " " * 50 + "\r", end="")  # æ¸…é™¤åŠ è½½åŠ¨ç”»
            
            if thread.is_alive():
                print("â° è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
                return None
                
            if exception[0]:
                print(f"âŒ APIè°ƒç”¨å‡ºé”™: {exception[0]}")
                return None
                
            return result[0]
            
        except Exception as e:
            print(f"âŒ å‘é€æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            return None


    def add_to_history(self, role: str, content: str):
        """
        æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²
        Args:
            role: è§’è‰² (user/assistant)
            content: æ¶ˆæ¯å†…å®¹
        """
        self.conversation_history.append({
            "role": role,
            "content": content,
            "timestamp": time.time()
        })
        self.update_token_count()


    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history.clear()
        self.current_tokens = 0
        print("ğŸ—‘ï¸  å¯¹è¯å†å²å·²æ¸…ç©º")


    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        help_text = """
ğŸ“– èŠå¤©å‘½ä»¤å¸®åŠ©:

åŸºæœ¬å¯¹è¯:
  - ç›´æ¥è¾“å…¥æ¶ˆæ¯ä¸Gemmaå¯¹è¯
  - Gemmaä¼šè®°ä½æ•´ä¸ªå¯¹è¯å†å²
  - æ”¯æŒæ–‡æœ¬ç”Ÿæˆå’Œå›¾åƒæ–‡æœ¬è½¬æ¢æ¨¡å‹

ç‰¹æ®Šå‘½ä»¤:
  /help     - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
  /clear    - æ¸…ç©ºå¯¹è¯å†å²
  /tokens   - æ˜¾ç¤ºå½“å‰tokenä½¿ç”¨æƒ…å†µ
  /history  - æ˜¾ç¤ºå¯¹è¯å†å²æ‘˜è¦
  /model    - æ˜¾ç¤ºå½“å‰æ¨¡å‹ä¿¡æ¯å’Œä¼˜åŒ–ç‰¹æ€§
  /exit     - é€€å‡ºèŠå¤©ç•Œé¢

ğŸš€ å¯ç”¨æ¨¡å‹ç±»å‹ (8ç§Googleå®˜æ–¹æ¨¡å‹):
  1-2. æ–‡æœ¬ç”Ÿæˆä¼˜åŒ– (1B, 270M) - 128Kè¾“å…¥/8Kè¾“å‡º
  3-5. å›¾åƒæ–‡æœ¬å¤„ç† (4B, 12B, 27B) - 128Kè¾“å…¥/8Kè¾“å‡º
  6-8. å®éªŒç‰ˆæœ¬ (E4Bç³»åˆ—) - 32Kè¾“å…¥è¾“å‡º

ğŸ’¡ æç¤º:
  - å¯¹è¯å†å²ä¼šå½±å“å›å¤è´¨é‡å’Œtokenæ¶ˆè€—
  - å½“tokenä½¿ç”¨è¿‡å¤šæ—¶å»ºè®®ä½¿ç”¨ /clear æ¸…ç†å†å²
  - ä½¿ç”¨ä¸­æ–‡æˆ–è‹±æ–‡éƒ½å¯ä»¥ä¸Gemmaå¯¹è¯
  - ä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„ä¼˜åŒ–ç‰¹æ€§ï¼Œé€‰æ‹©é€‚åˆçš„æ¨¡å‹
        """
        print(help_text)


    def show_history_summary(self):
        """æ˜¾ç¤ºå¯¹è¯å†å²æ‘˜è¦"""
        if not self.conversation_history:
            print("ğŸ“ æš‚æ— å¯¹è¯å†å²")
            return
            
        print(f"ğŸ“ å¯¹è¯å†å²æ‘˜è¦ (å…±{len(self.conversation_history)}æ¡æ¶ˆæ¯):")
        print("-" * 50)
        
        for i, msg in enumerate(self.conversation_history[-5:], 1):  # æ˜¾ç¤ºæœ€è¿‘5æ¡
            role = "ğŸ‘¤ ç”¨æˆ·" if msg["role"] == "user" else "ğŸ¤– Gemma"
            content_preview = msg["content"][:50] + "..." if len(msg["content"]) > 50 else msg["content"]
            print(f"{i}. {role}: {content_preview}")


    def show_model_info(self):
        """æ˜¾ç¤ºå½“å‰æ¨¡å‹ä¿¡æ¯"""
        # æŸ¥æ‰¾å½“å‰æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
        current_model_info = None
        for model_info in self.available_models.values():
            if model_info["name"] == self.model_name:
                current_model_info = model_info
                break
        
        print(f"ğŸ¤– å½“å‰æ¨¡å‹: {self.model_name}")
        if current_model_info:
            print(f"âš¡ ä¼˜åŒ–ç‰¹æ€§: {current_model_info['optimization']}")
        print(f"ğŸ‘¤ ç”¨æˆ·: {self.username}")
        print(f"ğŸ“Š è¾“å…¥Tokené™åˆ¶: {self.max_input_tokens:,}")
        print(f"ğŸ“¤ è¾“å‡ºTokené™åˆ¶: {self.max_output_tokens:,}")
        print(f"ğŸ’¬ å¯¹è¯è½®æ¬¡: {len(self.conversation_history)}")


    def start_chat(self):
        """å¼€å§‹èŠå¤©å¾ªç¯"""
        print("ğŸš€ èŠå¤©ç•Œé¢å·²å¯åŠ¨ï¼")
        print("ğŸ’¡ è¾“å…¥ /help æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
        print("ğŸ’¡ è¾“å…¥ /exit é€€å‡ºèŠå¤©")
        print("=" * 50)
        
        while True:
            try:
                # æ˜¾ç¤ºtokenä¿¡æ¯ï¼ˆæ¯5è½®å¯¹è¯æ˜¾ç¤ºä¸€æ¬¡ï¼‰
                if len(self.conversation_history) % 10 == 0 and len(self.conversation_history) > 0:
                    self.display_token_info()
                
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = input(f"\nğŸ‘¤ {self.username}: ").strip()
                
                if not user_input:
                    continue
                    
                # å¤„ç†ç‰¹æ®Šå‘½ä»¤
                if user_input.startswith('/'):
                    command = user_input.lower()
                    
                    if command == '/exit':
                        print("ğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨GemmaèŠå¤©ç•Œé¢")
                        break
                    elif command == '/help':
                        self.show_help()
                        continue
                    elif command == '/clear':
                        self.clear_history()
                        continue
                    elif command == '/tokens':
                        self.display_token_info()
                        continue
                    elif command == '/history':
                        self.show_history_summary()
                        continue
                    elif command == '/model':
                        self.show_model_info()
                        continue
                    else:
                        print("âŒ æœªçŸ¥å‘½ä»¤ï¼Œè¾“å…¥ /help æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
                        continue
                
                # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
                self.add_to_history("user", user_input)
                
                # å‘é€åˆ°Gemmaå¹¶è·å–å›å¤
                response = self.send_message_to_gemma(user_input)
                
                if response:
                    print(f"\nğŸ¤– Gemma: {response}")
                    self.add_to_history("assistant", response)
                else:
                    print("\nâŒ æŠ±æ­‰ï¼Œæ— æ³•è·å–å›å¤ï¼Œè¯·ç¨åé‡è¯•")
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ æ£€æµ‹åˆ°Ctrl+Cï¼Œæ­£åœ¨é€€å‡º...")
                break
            except Exception as e:
                print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
                print("ğŸ’¡ è¯·å°è¯•é‡æ–°è¾“å…¥æˆ–ä½¿ç”¨ /help æŸ¥çœ‹å¸®åŠ©")


    def run(self) -> bool:
        """
        è¿è¡ŒèŠå¤©ç•Œé¢
        Returns:
            æ˜¯å¦æˆåŠŸå¯åŠ¨
        """
        # 1. ç”¨æˆ·è®¤è¯
        if not self.setup_authentication():
            return False
            
        # 2. æ¨¡å‹é€‰æ‹©  
        if not self.setup_model_selection():
            return False
            
        # 3. å¼€å§‹èŠå¤©
        self.start_chat()
        return True




def main():
    """ä¸»å‡½æ•°"""
    try:
        chat = ChatInterface()
        chat.run()
    except Exception as e:
        print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
